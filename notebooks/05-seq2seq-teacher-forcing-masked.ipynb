{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy, random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# Some utility functions\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "global USE_CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1\n",
    "MAX_LEN = 20\n",
    "MIN_FREQ = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = '<s>', eos_token = '</s>') # only target needs BOS/EOS\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=DEVICE, repeat=False, sort_key=lambda x: len(x.src))\n",
    "\n",
    "def str_to_tensor(string, src_lang = DE):\n",
    "    string = string.split()\n",
    "    word_ids = [src_lang.vocab.stoi[word] for word in string]\n",
    "    word_tensor = Variable(torch.LongTensor(word_ids))\n",
    "    return word_tensor\n",
    "    \n",
    "def tensor_to_kaggle(tensor, trg_lang = EN):\n",
    "    return '|'.join([trg_lang.vocab.itos[word_id] for word_id in tensor])\n",
    "    \n",
    "def tensor_to_str(tensor, trg_lang = EN):\n",
    "    return ' '.join([trg_lang.vocab.itos[word_id] for word_id in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vsize, hidden_dim, n_layers = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.src_vsize = src_vsize\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(src_vsize, hidden_dim, padding_idx = DE.vocab.stoi[DE.pad_token])\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers = n_layers, batch_first = False)\n",
    "        \n",
    "    def forward(self, src_words):\n",
    "        embedded = self.embeddings(src_words)\n",
    "        out, hdn = self.lstm(embedded)\n",
    "        return out, hdn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, trg_vsize, n_layers = 1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.trg_vsize = trg_vsize\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(trg_vsize, hidden_dim, padding_idx = EN.vocab.stoi[EN.pad_token])\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers = n_layers, batch_first = False)\n",
    "        self.proj = nn.Linear(hidden_dim, trg_vsize)\n",
    "        \n",
    "    def forward(self, trg_words, hidden):\n",
    "        embedded = self.embeddings(trg_words)\n",
    "        out, hdn = self.lstm(embedded, hidden)\n",
    "        output = self.proj(out)\n",
    "        return output, hdn\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vsize, trg_vsize, hidden_dim, n_layers = 1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = Encoder(src_vsize, hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim, trg_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_iter, val_iter):\n",
    "        \"\"\" Initialize trainer class with Torchtext iterators \"\"\"\n",
    "        self.train_iter = train_iter\n",
    "        self.val_iter = val_iter\n",
    "        \n",
    "    def train(self, num_epochs, model, lr = 1e-3, clip = 5):\n",
    "        \"\"\" Train using Adam \"\"\"\n",
    "        weight = torch.FloatTensor(len(EN.vocab.itos)).fill_(1)\n",
    "        self.padding_id = EN.vocab.stoi[EN.pad_token]\n",
    "        weight[self.padding_id] = 0\n",
    "        weight = Variable(weight)\n",
    "        if USE_CUDA: \n",
    "            weight = weight.cuda()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight = weight, size_average = False)\n",
    "        parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(params = parameters, lr = lr)\n",
    "        best_ppl = 1e10\n",
    "        \n",
    "        all_losses = []\n",
    "        for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "\n",
    "            epoch_loss = []\n",
    "            \n",
    "            for batch in tqdm(self.train_iter):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                batch_loss = self.train_batch(batch, criterion, model, teacher_forcing_ratio = 0)\n",
    "                batch_loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss.append(batch_loss.data[0])\n",
    "                                \n",
    "                if len(epoch_loss) % 100 == 0:\n",
    "                    step = len(epoch_loss)\n",
    "                    cur_loss = np.mean(epoch_loss)\n",
    "                    train_ppl = np.exp(np.mean(epoch_loss))\n",
    "                    print('Step: {0} | Loss: {1} | Train PPL: {2}'.format(step, cur_loss, train_ppl))\n",
    "                    print('Wie würde eine solche Zukunft aussehen ? -->', self.translate('Wie würde eine solche Zukunft aussehen ?', model))\n",
    "                \n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            train_ppl = np.exp(epoch_loss)\n",
    "            val_ppl = self.validate(criterion, model)\n",
    "\n",
    "            print('Epoch: {0} | Loss: {1} | Train PPL: {2} | Val PPL: {3}'.format(epoch, epoch_loss, train_ppl, val_ppl))\n",
    "            all_losses.append(epoch_loss)\n",
    "            \n",
    "            # early stopping\n",
    "            if val_ppl < best_ppl:\n",
    "                best_ppl = val_ppl\n",
    "                best_model = model\n",
    "        \n",
    "        torch.save(best_model.cpu(), best_model.__class__.__name__ + \".pth\")\n",
    "        return best_model.cpu(), all_losses        \n",
    "                \n",
    "    def train_batch(self, batch, criterion, model, teacher_forcing_ratio = 0):\n",
    "        \"\"\" Compute training batch using teacher forcing \"\"\"\n",
    "        # Initialize batch loss to zero, get size of target sentences\n",
    "        loss = 0\n",
    "        target_length = batch.trg.size()[0]\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, decoder_hidden = model.encoder(batch.src)\n",
    "\n",
    "        # Prepare input and output variables\n",
    "        use_teacher_forcing = random.random() > teacher_forcing_ratio\n",
    "        if use_teacher_forcing:\n",
    "\n",
    "            # With teacher forcing, we use the previous true target as the next word input.\n",
    "            # This allows us to batch the softmax, resulting in large speed-ups.\n",
    "            shift = Variable(torch.LongTensor(batch.batch_size).fill_(1)).unsqueeze(0)\n",
    "            if USE_CUDA:\n",
    "                shift = shift.cuda()\n",
    "\n",
    "            # Run words through encoder\n",
    "            encoder_outputs, encoder_hidden = model.encoder(batch.src)\n",
    "\n",
    "            # Get outputs for batch, using encoder hidden as initialization for decoder hidden\n",
    "            decoder_outputs, decoder_hidden = model.decoder(batch.trg, encoder_hidden)\n",
    "\n",
    "            # Reshape outputs, add shift tensor to targets\n",
    "            preds = decoder_outputs.view(target_length * batch.batch_size, -1)\n",
    "            targets = torch.cat((batch.trg[1:], shift), dim = 0).view(-1)\n",
    "\n",
    "            # Compute loss in a batch (more efficient than loop)\n",
    "            num_words = targets.ne(self.padding_id).float().sum()\n",
    "            loss = criterion(preds, targets)\n",
    "            loss /= num_words\n",
    "            return loss\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # Without teacher forcing: use network's own prediction as the next input\n",
    "            decoder_inputs = batch.trg[0, :].unsqueeze(0)\n",
    "            for trg_word_idx in range(target_length-1):\n",
    "                decoder_output, decoder_hidden = model.decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "                # Get most likely word index (highest value) from output\n",
    "                _, topk_words_idx = decoder_output.data.topk(1, dim = 2)\n",
    "\n",
    "                # Chosen word is next input\n",
    "                decoder_inputs = Variable(topk_words_idx).squeeze(2)\n",
    "                if USE_CUDA: \n",
    "                    decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "                # Compute loss for all words in batch\n",
    "                num_words = batch.trg[trg_word_idx, :].ne(trainer.padding_id).float().sum()\n",
    "                loss += (criterion(decoder_output.squeeze(0), batch.trg[trg_word_idx+1, :]) / num_words) if num_words.data[0] > 0 else 0\n",
    "                \n",
    "        loss /= batch.batch_size\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def translate(self, string, model, maxlength = None):  \n",
    "        \"\"\" Predict translation for an input string \"\"\"\n",
    "        # Make string a tensor\n",
    "        tensor = str_to_tensor(string)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        if USE_CUDA:\n",
    "            tensor = tensor.cuda()\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, decoder_hidden = model.encoder(tensor)\n",
    "\n",
    "        # First token must always start of sentence <s>\n",
    "        decoder_inputs = Variable(torch.LongTensor([EN.vocab.stoi[EN.init_token]])).unsqueeze(0)\n",
    "        if USE_CUDA: \n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # if no maxlength, let it be 3*length original\n",
    "        maxlength = maxlength if maxlength else 3 * tensor.shape[0]\n",
    "        out_string = []\n",
    "\n",
    "        # Predict words until an <eos> token or maxlength\n",
    "        for trg_word_idx in range(maxlength):\n",
    "            decoder_output, decoder_hidden = model.decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "            # Get most likely word index (highest value) from output\n",
    "            prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "            top_probs, top_word_idx = prob_dist.data.topk(1, dim = 2)\n",
    "            ni = top_word_idx.squeeze(0)\n",
    "\n",
    "            decoder_inputs = Variable(ni) # Chosen word is next input\n",
    "            out_string.append(ni[0][0])\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni[0][0] == EN.vocab.stoi[EN.eos_token]: \n",
    "                break\n",
    "\n",
    "        out_string = tensor_to_str(out_string)\n",
    "        return out_string\n",
    "    \n",
    "    def evaluate_kaggle(self, string, model, ngrams = 3, context = 0, top_k = 100):\n",
    "        \"\"\" Beam search the best starting trigrams for Kaggle input sentences. 'beam_size' here is more like add'l context.. \"\"\"\n",
    "        # Convert string to tensor for embedding lookups\n",
    "        tensor = str_to_tensor(string)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        if USE_CUDA:\n",
    "            tensor = tensor.cuda()\n",
    "\n",
    "        # Run words through encoder to get init hidden for decoder\n",
    "        encoder_outputs, encoder_hidden = model.encoder(tensor)\n",
    "\n",
    "        # Start collecting hiddens, prepare initial input variables\n",
    "        decoder_inputs = Variable(torch.LongTensor([EN.vocab.stoi[EN.init_token]])).unsqueeze(0)\n",
    "        if USE_CUDA: \n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # Compute the top K first words, so that we have something to work with\n",
    "        decoder_output, decoder_hidden = model.decoder(decoder_inputs, encoder_hidden)\n",
    "        prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "        top_probs, top_word_idx = prob_dist.data.topk(top_k, dim = 2)\n",
    "        decoder_inputs = Variable(top_word_idx)\n",
    "        if USE_CUDA:\n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # Begin table to keep our outputs, output_probs\n",
    "        outputs = [[word] for word in list(decoder_inputs.data[0][0])]\n",
    "        output_probs = list(top_probs[0][0])\n",
    "\n",
    "        # For using the correct hidden to predict next word. Initially it is 100x copy\n",
    "        all_hiddens = [decoder_hidden for _ in range(top_k)]\n",
    "\n",
    "        # Get top_k beams for \n",
    "        for trg_word_idx in range(1, ngrams+context):\n",
    "            beam_search_idx, beam_search_probs = [], []\n",
    "            for k in range(top_k):\n",
    "                decoder_output, new_hdn = model.decoder(decoder_inputs[:, :, k], all_hiddens[k])\n",
    "                prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "                top_probs, top_word_idx = prob_dist.data.topk(top_k, dim = 2)\n",
    "                beam_search_idx.append(list(top_word_idx[0][0]))\n",
    "                beam_search_probs.append(list(top_probs[0][0]))\n",
    "                all_hiddens[k] = new_hdn\n",
    "\n",
    "            # Top K words idx\n",
    "            next_word_idx = np.argsort(np.hstack(beam_search_probs))[::-1][:top_k] \n",
    "\n",
    "            # Backpointers to the input word that each top word was drawn from\n",
    "            back_pointers = [int(np.floor(word / top_k)) for word in next_word_idx] \n",
    "\n",
    "            # Update output list with new decoder inputs and their corresponding probabilities\n",
    "            next_words = [np.hstack(beam_search_idx)[ids] for ids in next_word_idx]\n",
    "            next_probs = [np.hstack(beam_search_probs)[ids] for ids in next_word_idx]\n",
    "            decoder_inputs = Variable(torch.LongTensor([int(word) for word in next_words])).unsqueeze(0).unsqueeze(0)\n",
    "            if USE_CUDA:\n",
    "                decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "            # update hiddens, outputs\n",
    "            all_hiddens = [all_hiddens[pointer] for pointer in back_pointers]\n",
    "            outputs = [outputs[pointer] + [word] for pointer, word in zip(back_pointers, next_words)]\n",
    "            output_probs = [output_probs[pointer] + new_p for pointer, new_p in zip(back_pointers, next_probs)]\n",
    "\n",
    "        prob_sort_idx = np.argsort(output_probs)[::-1]\n",
    "        outputs = [outputs[idx] for idx in prob_sort_idx]\n",
    "        outputs = [output[:ngrams] for output in outputs]\n",
    "        out = [tensor_to_kaggle(tsr) for tsr in outputs]\n",
    "        return ' '.join(out)\n",
    "        \n",
    "    def validate(self, criterion, model):\n",
    "        \"\"\" Compute validation set perplexity \"\"\"\n",
    "        loss = []\n",
    "        for batch in tqdm(self.val_iter):\n",
    "            batch_loss = self.train_batch(batch, criterion, model)\n",
    "            loss.append(batch_loss.data[0])\n",
    "        \n",
    "        val_ppl = np.exp(np.mean(loss))\n",
    "        return val_ppl\n",
    "    \n",
    "    def write_kaggle(self, test_file, model):\n",
    "        \"\"\" Write outputs to kaggle \"\"\"\n",
    "        with open(test_file, 'r') as fh:\n",
    "            datasource = fh.read().splitlines()\n",
    "            \n",
    "        print('Evaluating on {0}...'.format(test_file))\n",
    "        with open('output.txt', 'w') as fh:\n",
    "            fh.write('id,word\\n')\n",
    "            for idx, string in tqdm(enumerate(datasource)):\n",
    "                output = self.evaluate_kaggle(string, model)\n",
    "                output = str(idx+1) + ',' + self.escape_kaggle(output) + '\\n'\n",
    "                fh.write(output)\n",
    "        print('File saved.')\n",
    "        \n",
    "    def escape_kaggle(self, l):\n",
    "        \"\"\" So kaggle doesn't yell at you when submitting results \"\"\"\n",
    "        return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.50it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.55it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.55it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.58it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.66it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.69it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  1.69it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.67it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.47it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.78it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.77it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.93it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.23it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.08it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.83it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.39it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.14it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  7.93it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.76it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.59it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:02<00:00,  7.47it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.33it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.22it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.21it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:08<01:15,  8.40s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 9.017608547210694 | Train PPL: 8247.031094360527 | Val PPL: 3545.5138889986415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.59it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.64it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.67it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.64it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.64it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.65it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  1.66it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.67it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.70it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 10.94it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.35it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.37it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.59it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  8.95it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  8.84it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.58it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.18it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  7.87it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  7.69it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.56it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.41it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:02<00:00,  7.30it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.21it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.14it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.09it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:16<01:07,  8.40s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 6.9129250049591064 | Train PPL: 1005.1831120678406 | Val PPL: 183.86663452568501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.72it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.84it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.79it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.83it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.84it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.91it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.90it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.84it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.27it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.36it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.34it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.69it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.12it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  8.86it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.54it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.14it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  7.91it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  7.74it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.62it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.48it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.28it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.20it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.19it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:24<00:57,  8.25s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 4.798767352104187 | Train PPL: 121.36073023307311 | Val PPL: 76.66816568504365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:04,  2.14it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.47it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.23it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.07it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.99it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.93it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.97it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.92it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.89it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.87it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.28it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.63it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.66it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.91it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.28it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.04it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.72it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.29it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.09it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  7.92it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.72it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.62it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:02<00:00,  7.48it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.30it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.29it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:32<00:48,  8.15s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 3.8190979957580566 | Train PPL: 45.56309167797375 | Val PPL: 57.109087457576045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:04,  1.90it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.74it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.73it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.72it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.75it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.77it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.77it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.34it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.58it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.62it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.91it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.21it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.09it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.78it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.40it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.17it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  7.97it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.84it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.69it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.59it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.48it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.39it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.40it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:40<00:40,  8.13s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 4.163698053359985 | Train PPL: 64.3089011581296 | Val PPL: 49.40731267176248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.66it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.86it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.81it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.86it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.81it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.81it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.78it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.79it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.78it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.77it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.51it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.56it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.68it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.94it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.36it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.15it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.84it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.47it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.20it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  8.05it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.88it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.73it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.61it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.51it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.42it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.41it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:48<00:32,  8.13s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 4.327410387992859 | Train PPL: 75.74787475776951 | Val PPL: 46.31315406976407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.67it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.65it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.85it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.80it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.82it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.88it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.85it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.83it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.80it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.62it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.96it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.82it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.98it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.40it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.27it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:00<00:00,  9.04it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.64it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.40it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  8.18it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.97it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.82it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.67it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.56it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.45it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.45it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:56<00:24,  8.10s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 3.862162232398987 | Train PPL: 47.56809352559382 | Val PPL: 43.67900312246136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.64it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.02it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.98it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.90it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.85it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.86it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.83it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.80it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.82it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.83it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.66it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.59it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.70it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.98it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.45it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.21it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.99it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.51it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.31it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  8.13it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.94it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.80it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.65it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.55it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.44it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.44it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [01:04<00:16,  8.08s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 3.8789171457290648 | Train PPL: 48.37180708346625 | Val PPL: 41.799916744638104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.71it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.66it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.70it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.71it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.70it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.76it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.82it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.81it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 11.46it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.85it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.85it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.94it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.28it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.02it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.85it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.41it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.23it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  8.05it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.89it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.72it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.60it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.49it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.38it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.39it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:12<00:08,  8.06s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 3.7423595905303957 | Train PPL: 42.197441469975004 | Val PPL: 40.59977096423215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.62it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.72it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.71it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.69it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.73it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.77it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  1.85it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.86it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  1.90it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 2/18 [00:00<00:01, 10.69it/s]\u001b[A\n",
      " 17%|█▋        | 3/18 [00:00<00:01, 10.26it/s]\u001b[A\n",
      " 28%|██▊       | 5/18 [00:00<00:01, 10.46it/s]\u001b[A\n",
      " 33%|███▎      | 6/18 [00:00<00:01,  9.74it/s]\u001b[A\n",
      " 39%|███▉      | 7/18 [00:00<00:01,  9.24it/s]\u001b[A\n",
      " 44%|████▍     | 8/18 [00:00<00:01,  9.05it/s]\u001b[A\n",
      " 50%|█████     | 9/18 [00:01<00:01,  8.82it/s]\u001b[A\n",
      " 56%|█████▌    | 10/18 [00:01<00:00,  8.45it/s]\u001b[A\n",
      " 61%|██████    | 11/18 [00:01<00:00,  8.21it/s]\u001b[A\n",
      " 67%|██████▋   | 12/18 [00:01<00:00,  8.03it/s]\u001b[A\n",
      " 72%|███████▏  | 13/18 [00:01<00:00,  7.86it/s]\u001b[A\n",
      " 78%|███████▊  | 14/18 [00:01<00:00,  7.72it/s]\u001b[A\n",
      " 83%|████████▎ | 15/18 [00:01<00:00,  7.58it/s]\u001b[A\n",
      " 89%|████████▉ | 16/18 [00:02<00:00,  7.50it/s]\u001b[A\n",
      " 94%|█████████▍| 17/18 [00:02<00:00,  7.42it/s]\u001b[A\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.42it/s]\u001b[A\n",
      "100%|██████████| 10/10 [01:20<00:00,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 3.4612477302551268 | Train PPL: 31.85670029602002 | Val PPL: 39.752187507279146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/sob/Desktop/cs287/homeworks/env/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/sob/Desktop/cs287/homeworks/env/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/sob/Desktop/cs287/homeworks/env/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(src_vsize = len(DE.vocab.itos), trg_vsize = len(EN.vocab.itos), hidden_dim = 200)\n",
    "trainer = Trainer(train_iter, val_iter)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "print('Using cuda: ', np.all([parameter.is_cuda for parameter in model.parameters()]))\n",
    "model, all_losses = trainer.train(15, model)\n",
    "if USE_CUDA: model = model.cuda() # don't know why but this will throw errors on GPU otherwise if not re-cuda'd\n",
    "trainer.write_kaggle('../data/source_test.txt', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
