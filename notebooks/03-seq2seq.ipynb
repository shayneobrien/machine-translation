{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy, random, argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# Some utility functions\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "global USE_CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1\n",
    "MAX_LEN = 20\n",
    "MIN_FREQ = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = '<s>', eos_token = '</s>') # only target needs BOS/EOS\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=DEVICE, repeat=False, sort_key=lambda x: len(x.src))\n",
    "\n",
    "def str_to_tensor(string, src_lang = DE):\n",
    "    string = string.split()\n",
    "    word_ids = [src_lang.vocab.stoi[word] for word in string]\n",
    "    word_tensor = Variable(torch.LongTensor(word_ids))\n",
    "    if USE_CUDA:\n",
    "        return word_tensor.cuda()\n",
    "    else:\n",
    "        return word_tensor\n",
    "    \n",
    "def tensor_to_kaggle(tensor, trg_lang = EN):\n",
    "    return '|'.join([trg_lang.vocab.itos[word_id] for word_id in tensor])\n",
    "    \n",
    "def tensor_to_str(tensor, trg_lang = EN):\n",
    "    return ' '.join([trg_lang.vocab.itos[word_id] for word_id in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vsize, hidden_dim, n_layers = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.src_vsize = src_vsize\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(src_vsize, hidden_dim, padding_idx = DE.vocab.stoi[DE.pad_token])\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers = n_layers, batch_first = False)\n",
    "        \n",
    "    def forward(self, src_words):\n",
    "        embedded = self.embeddings(src_words)\n",
    "        out, hdn = self.lstm(embedded)\n",
    "        return out, hdn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, trg_vsize, n_layers = 1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.trg_vsize = trg_vsize\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(trg_vsize, hidden_dim, padding_idx = EN.vocab.stoi[EN.pad_token])\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers = n_layers, batch_first = False)\n",
    "        self.proj = nn.Linear(hidden_dim, trg_vsize)\n",
    "        \n",
    "    def forward(self, trg_words, hidden):\n",
    "        embedded = self.embeddings(trg_words)\n",
    "        out, hdn = self.lstm(embedded, hidden)\n",
    "        output = self.proj(out)\n",
    "        return output, hdn\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vsize, trg_vsize, hidden_dim, n_layers = 1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = Encoder(src_vsize, hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim, trg_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_iter, val_iter):\n",
    "        \"\"\" Initialize trainer class with Torchtext iterators \"\"\"\n",
    "        self.train_iter = train_iter\n",
    "        self.val_iter = val_iter\n",
    "        \n",
    "    def train(self, num_epochs, model, lr = 1e-3, clip = 5):\n",
    "        \"\"\" Train using Adam \"\"\"\n",
    "        best_ppl = 75\n",
    "        parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(params = parameters, lr = lr)\n",
    "        \n",
    "        all_losses = []\n",
    "        for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "\n",
    "            epoch_loss = []\n",
    "            for batch in tqdm(self.train_iter):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                batch_loss = self.train_batch(batch, model)\n",
    "                batch_loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss.append(batch_loss.data[0])\n",
    "                                \n",
    "                if len(epoch_loss) % 100 == 0:\n",
    "                    step = len(epoch_loss)\n",
    "                    cur_loss = np.mean(epoch_loss)\n",
    "                    train_ppl = np.exp(np.mean(epoch_loss))\n",
    "                    print('Step: {0} | Loss: {1} | Train PPL: {2}'.format(step, cur_loss, train_ppl))\n",
    "                    print('Wie würde eine solche Zukunft aussehen ? -->', self.translate('Wie würde eine solche Zukunft aussehen ?', model))\n",
    "                \n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            train_ppl = np.exp(epoch_loss)\n",
    "            val_ppl = self.validate(model)\n",
    "\n",
    "            print('Epoch: {0} | Loss: {1} | Train PPL: {2} | Val PPL: {3}'.format(epoch, epoch_loss, train_ppl, val_ppl))\n",
    "            all_losses.append(epoch_loss)\n",
    "            \n",
    "            # early stopping\n",
    "            if val_ppl < best_ppl:\n",
    "                best_ppl = val_ppl\n",
    "                best_model = model\n",
    "        \n",
    "        torch.save(best_model.cpu(), best_model.__class__.__name__ + \".pth\")\n",
    "        return best_model.cpu(), all_losses        \n",
    "                \n",
    "    def train_batch(self, batch, model):\n",
    "        \"\"\" Get train batch using teacher forcing (prev. true target is next word input) always. \n",
    "            Results in large speed-up. \"\"\"\n",
    "        \n",
    "        # Get target length, create shift tensor (since we take word n-1 to predict word n)\n",
    "        target_length = batch.trg.size()[0]\n",
    "        shift = Variable(torch.LongTensor(batch.batch_size).fill_(1)).unsqueeze(0)\n",
    "        if USE_CUDA:\n",
    "            shift = shift.cuda()\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, encoder_hidden = model.encoder(batch.src)\n",
    "\n",
    "        # Get outputs for batch, using encoder hidden as initialization for decoder hidden\n",
    "        decoder_outputs, decoder_hidden = model.decoder(batch.trg, encoder_hidden)\n",
    "\n",
    "        # Reshape outputs, add shift tensor to targets\n",
    "        preds = decoder_outputs.view(target_length * batch.batch_size, -1)\n",
    "        targets = torch.cat((batch.trg[1:], shift), dim = 0).view(-1)\n",
    "\n",
    "        # Compute loss in a batch (more efficient than loop)\n",
    "        loss = F.cross_entropy(preds, targets)\n",
    "        return loss\n",
    "    \n",
    "    def translate(self, string, model, maxlength = None):  \n",
    "        \"\"\" Predict translation for an input string \"\"\"\n",
    "        # Make string a tensor\n",
    "        tensor = str_to_tensor(string)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        if USE_CUDA:\n",
    "            tensor = tensor.cuda()\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, decoder_hidden = model.encoder(tensor)\n",
    "\n",
    "        # First token must always start of sentence <s>\n",
    "        decoder_inputs = Variable(torch.LongTensor([EN.vocab.stoi[EN.init_token]])).unsqueeze(0)\n",
    "        if USE_CUDA: \n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # if no maxlength, let it be 3*length original\n",
    "        maxlength = maxlength if maxlength else 3 * tensor.shape[0]\n",
    "        out_string = []\n",
    "\n",
    "        # Predict words until an <eos> token or maxlength\n",
    "        for trg_word_idx in range(maxlength):\n",
    "            decoder_output, decoder_hidden = model.decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "            # Get most likely word index (highest value) from output\n",
    "            prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "            top_probs, top_word_idx = prob_dist.data.topk(1, dim = 2)\n",
    "            ni = top_word_idx.squeeze(0)\n",
    "\n",
    "            decoder_inputs = Variable(ni) # Chosen word is next input\n",
    "            out_string.append(ni[0][0])\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni[0][0] == EN.vocab.stoi[EN.eos_token]: \n",
    "                break\n",
    "\n",
    "        out_string = tensor_to_str(out_string)\n",
    "        return out_string\n",
    "    \n",
    "    def evaluate_kaggle(self, string, model, ngrams = 3, context = 0, top_k = 100):\n",
    "        \"\"\" Beam search the best starting trigrams for Kaggle input sentences \"\"\"\n",
    "        # Convert string to tensor for embedding lookups\n",
    "        tensor = str_to_tensor(string)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        if USE_CUDA:\n",
    "            tensor = tensor.cuda()\n",
    "\n",
    "        # Run words through encoder to get init hidden for decoder\n",
    "        encoder_outputs, encoder_hidden = model.encoder(tensor)\n",
    "\n",
    "        # Start collecting hiddens, prepare initial input variables\n",
    "        decoder_inputs = Variable(torch.LongTensor([EN.vocab.stoi[EN.init_token]])).unsqueeze(0)\n",
    "        if USE_CUDA: \n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # Compute the top K first words, so that we have something to work with\n",
    "        decoder_output, decoder_hidden = model.decoder(decoder_inputs, encoder_hidden)\n",
    "        prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "        top_probs, top_word_idx = prob_dist.data.topk(top_k, dim = 2)\n",
    "        decoder_inputs = Variable(top_word_idx)\n",
    "        if USE_CUDA:\n",
    "            decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "        # Begin table to keep our outputs, output_probs\n",
    "        outputs = [[word] for word in list(decoder_inputs.data[0][0])]\n",
    "        output_probs = list(top_probs[0][0])\n",
    "\n",
    "        # For using the correct hidden to predict next word. Initially it is 100x copy\n",
    "        all_hiddens = [decoder_hidden for _ in range(top_k)]\n",
    "\n",
    "        # Get top_k beams for \n",
    "        for trg_word_idx in range(1, ngrams+context):\n",
    "            beam_search_idx, beam_search_probs = [], []\n",
    "            for k in range(top_k):\n",
    "                decoder_output, new_hdn = model.decoder(decoder_inputs[:, :, k], all_hiddens[k])\n",
    "                prob_dist = F.log_softmax(decoder_output, dim = 2)\n",
    "                top_probs, top_word_idx = prob_dist.data.topk(top_k, dim = 2)\n",
    "                beam_search_idx.append(list(top_word_idx[0][0]))\n",
    "                beam_search_probs.append(list(top_probs[0][0]))\n",
    "                all_hiddens[k] = new_hdn\n",
    "\n",
    "            # Top K words idx\n",
    "            next_word_idx = np.argsort(np.hstack(beam_search_probs))[::-1][:top_k] \n",
    "\n",
    "            # Backpointers to the input word that each top word was drawn from\n",
    "            back_pointers = [int(np.floor(word / top_k)) for word in next_word_idx] \n",
    "\n",
    "            # Update output list with new decoder inputs and their corresponding probabilities\n",
    "            next_words = [np.hstack(beam_search_idx)[ids] for ids in next_word_idx]\n",
    "            next_probs = [np.hstack(beam_search_probs)[ids] for ids in next_word_idx]\n",
    "            decoder_inputs = Variable(torch.LongTensor([int(word) for word in next_words])).unsqueeze(0).unsqueeze(0)\n",
    "            if USE_CUDA:\n",
    "                decoder_inputs = decoder_inputs.cuda()\n",
    "\n",
    "            # update hiddens, outputs\n",
    "            all_hiddens = [all_hiddens[pointer] for pointer in back_pointers]\n",
    "            outputs = [outputs[pointer] + [word] for pointer, word in zip(back_pointers, next_words)]\n",
    "            output_probs = [output_probs[pointer] + new_p for pointer, new_p in zip(back_pointers, next_probs)]\n",
    "\n",
    "        prob_sort_idx = np.argsort(output_probs)[::-1]\n",
    "        outputs = [outputs[idx] for idx in prob_sort_idx]\n",
    "        outputs = [output[:ngrams] for output in outputs]\n",
    "        out = [tensor_to_kaggle(tsr) for tsr in outputs]\n",
    "        return ' '.join(out)\n",
    "        \n",
    "    def validate(self, model):\n",
    "        \"\"\" Compute validation set perplexity \"\"\"\n",
    "        loss = []\n",
    "        for batch in tqdm(self.val_iter):\n",
    "            batch_loss = self.train_batch(batch, model)\n",
    "            loss.append(batch_loss.data[0])\n",
    "        \n",
    "        val_ppl = np.exp(np.mean(loss))\n",
    "        return val_ppl\n",
    "    \n",
    "    def write_kaggle(self, test_file, model):\n",
    "        \"\"\" Write outputs to kaggle \"\"\"\n",
    "        with open(test_file, 'r') as fh:\n",
    "            datasource = fh.read().splitlines()\n",
    "        \n",
    "        print('Evaluating on {0}...'.format(test_file))\n",
    "        with open('output.txt', 'w') as fh:\n",
    "            fh.write('id,word\\n')\n",
    "            for idx, string in tqdm(enumerate(datasource)):\n",
    "                output = self.evaluate_kaggle(string, model)\n",
    "                output = str(idx+1) + ',' + self.escape_kaggle(output) + '\\n'\n",
    "                fh.write(output)\n",
    "        print('File saved.')\n",
    "        \n",
    "    def escape_kaggle(self, l):\n",
    "        \"\"\" So kaggle doesn't yell at you when submitting results \"\"\"\n",
    "        return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'taught quantum software scanned comment cash injection Austin confusion 1976 rehab completely argue naughty ROD Ma given vital In emergency peacekeeping'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(src_vsize = len(DE.vocab.itos), trg_vsize = len(EN.vocab.itos), hidden_dim = 200)\n",
    "trainer = Trainer(train_iter, val_iter)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "print('Using cuda: ', np.all([parameter.is_cuda for parameter in model.parameters()]))\n",
    "model, all_losses = trainer.train(15, model)\n",
    "trainer.write_kaggle('../data/source_test.txt', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
